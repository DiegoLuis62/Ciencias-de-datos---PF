{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DiegoLuis62/Ciencias-de-datos---PF/blob/main/Heart_Disease_Cleanse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpLXvYrfxLrH"
      },
      "source": [
        "# **1. Carga y exploración inicial del dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "7tQt43dEhg-T"
      },
      "outputs": [],
      "source": [
        "# Bibliotecas estándar\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Manipulación de datos\n",
        "import pandas as pd\n",
        "\n",
        "# Visualización\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    precision_recall_curve,\n",
        "    average_precision_score\n",
        ")\n",
        "from sklearn.impute import SimpleImputer\n",
        "import lightgbm as lgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTa_8GeYhvLT"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "dataset_path = \"/content/drive/My Drive/10 semestre/PF-Dataset\"\n",
        "df = pd.read_csv(dataset_path + \"/heart_2022_with_nans.csv\")\n",
        "# Link del dataset : https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raj42tp_lk4O"
      },
      "outputs": [],
      "source": [
        "print(df.shape)\n",
        "print(\"\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7dg19PPmj1F"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bBRHPktwJuv"
      },
      "source": [
        "# **2. Análisis de valores nulos: cantidad, porcentaje y tipo de datos**\n",
        "\n",
        "En este análisis, se identifican las columnas que contienen valores nulos dentro del dataset. Se calcula la cantidad y el porcentaje de datos faltantes en cada variable, además de determinar su tipo de dato (numérico o categórico). Esto permite evaluar el impacto de los valores ausentes y definir estrategias adecuadas para la limpieza y el tratamiento de los datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WA1cEZiiokl-"
      },
      "outputs": [],
      "source": [
        "missing_values = df.isnull().sum()\n",
        "\n",
        "# Filtrar solo las columnas con NaN\n",
        "missing_columns = missing_values[missing_values > 0]\n",
        "\n",
        "# Obtener los tipos de datos de esas columnas\n",
        "dtypes = df.dtypes[missing_columns.index]\n",
        "\n",
        "# Crear un DataFrame con la información\n",
        "missing_info = pd.DataFrame({\n",
        "    \"Missing Values\": missing_columns,\n",
        "    \"Percentage\": (missing_columns / len(df)) * 100,\n",
        "    \"Data Type\": dtypes\n",
        "})\n",
        "\n",
        "\n",
        "missing_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DB1XmIPEW4nV"
      },
      "outputs": [],
      "source": [
        "for col in df: # Verificar si hay una columna o atributo que no aporte, ejemplo si tiene 1 subnivel, no nos sirve\n",
        "  print(f\"Columna {col}: {df[col].nunique() } subniveles \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiWCHnycktOj"
      },
      "outputs": [],
      "source": [
        "# Verificar cantidad de filas duplicadas en el dataset original\n",
        "duplicados = df.duplicated().sum()\n",
        "\n",
        "print(f\"Número de filas duplicadas en el dataset original: {duplicados}\")\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Acabo de eliminar 157 datos duplicados del dataset original para evitar tener que eliminarlos uno por uno al copiar los demás datasets."
      ],
      "metadata": {
        "id": "jOnWzxeB6lcX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1p0mQPl1BiM0"
      },
      "source": [
        "#**3) Limpieza de los Datasets**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1-9kHCFDKHz"
      },
      "source": [
        "#Limpieza de Dataset por Eliminación\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yI0bYhDvwFkk"
      },
      "outputs": [],
      "source": [
        "df_cleaned = df.dropna()  # Elimina filas con valores nulos\n",
        "print(f\"Filas originales: {df.shape[0]}\")\n",
        "print(f\"Filas después de limpieza: {df_cleaned.shape[0]}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-PC8UyrwsVE"
      },
      "source": [
        "#Limpieza de Dataset Usando Moda, Mediana"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2Ck33hEy7XV"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "df_imputado = df.copy()\n",
        "\n",
        "# Identificar variables categóricas y numéricas\n",
        "categorical_vars = df_imputado.select_dtypes(include=['object']).columns\n",
        "binary_vars = [var for var in categorical_vars if df_imputado[var].nunique() == 2]  # Variables binarias\n",
        "num_vars = df_imputado.select_dtypes(include=['float64']).columns\n",
        "\n",
        "# Imputación con moda para categóricas y binarias\n",
        "imputer_moda = SimpleImputer(strategy=\"most_frequent\")\n",
        "df_imputado[categorical_vars] = imputer_moda.fit_transform(df_imputado[categorical_vars])\n",
        "\n",
        "# Imputación con mediana para variables numéricas\n",
        "imputer_mediana = SimpleImputer(strategy=\"median\")\n",
        "df_imputado[num_vars] = imputer_mediana.fit_transform(df_imputado[num_vars])\n",
        "\n",
        "# Guardar dataset imputado\n",
        "df_imputado.to_csv(\"dataset_imputado.csv\", index=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Filas originales: {df.shape[0]}\")\n",
        "print(f\"Filas después de imputar: {df_imputado.shape[0]}\")"
      ],
      "metadata": {
        "id": "R17hzzKh4-fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4) Comparación de columnas numéricas del dataset original, eliminación de datos nulos e imputación.**"
      ],
      "metadata": {
        "id": "REqrJahj5UQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset con datos Originales**"
      ],
      "metadata": {
        "id": "KWZso2fg6H9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.describe()\n"
      ],
      "metadata": {
        "id": "ww8YfmXA5Jr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset con datos nulos eliminados**"
      ],
      "metadata": {
        "id": "aDQGC7JA5235"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8HNBfqwYuY2"
      },
      "outputs": [],
      "source": [
        "df_cleaned.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset con datos Imputados**"
      ],
      "metadata": {
        "id": "DQCLocBr6MCl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "883g4M82zj0X"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_imputado.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfcTILek0y8Y"
      },
      "source": [
        "# **Análisis Comparativo de Métodos para el Manejo de Datos Faltantes**\n",
        "\n",
        "En el presente análisis, se compararon dos estrategias para tratar los datos faltantes en el dataset: la eliminación de registros con valores nulos y la imputación mediante la moda (para datos categóricos) y la mediana (para datos numéricos). A continuación, se presentan los hallazgos clave.\n",
        "\n",
        "# 1️⃣ Eliminación de Datos Faltantes\n",
        "\n",
        "Se redujo significativamente la cantidad de datos, pasando de ~434,205 registros a 246,022 en algunas columnas, representando una pérdida aproximada del 43.3% de la información.\n",
        "\n",
        "La media de variables clave, como el Índice de Masa Corporal (BMI), pasó de 28.53 en el dataset original a 28.67 tras la eliminación de datos, lo que sugiere que los valores eliminados tenían una distribución diferente al conjunto de datos restante.\n",
        "\n",
        "Se observó una disminución en la dispersión de los datos (medida a través de la desviación estándar), lo que indica una posible pérdida de variabilidad en la muestra.\n",
        "\n",
        "# 2️⃣ Imputación con Moda y Mediana\n",
        "\n",
        "Se conservaron 445,132 registros, manteniendo la totalidad de la muestra sin pérdida de información.\n",
        "\n",
        "La media del BMI fue 28.41, mucho más cercana al valor original (28.53) en comparación con la eliminación de datos.\n",
        "\n",
        "La desviación estándar disminuyó ligeramente en comparación con el dataset original, pero sin alterar significativamente la variabilidad inherente a los datos.\n",
        "\n",
        "# 📌 Conclusión\n",
        "\n",
        "El análisis comparativo evidencia que la eliminación de datos genera una pérdida considerable de información y puede introducir sesgos en la muestra al alterar las estadísticas descriptivas de las variables. En contraste, la imputación con moda y mediana preserva la estructura del dataset y mantiene los valores estadísticos más cercanos a los originales.\n",
        "\n",
        "Por lo tanto, para evitar la pérdida de información valiosa y minimizar el sesgo en futuros análisis, se recomienda la imputación de valores faltantes en lugar de la eliminación de registros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbBj0wv-mkLg"
      },
      "source": [
        "# **5) Análisis de datos**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Suponiendo que tienes tu DataFrame llamado 'df'\n",
        "numerical_columns = [\n",
        "    \"PhysicalHealthDays\", \"MentalHealthDays\", \"SleepHours\",\n",
        "    \"HeightInMeters\", \"WeightInKilograms\", \"BMI\"\n",
        "]\n",
        "\n",
        "# Crear boxplots para cada columna numérica\n",
        "for col in numerical_columns:\n",
        "    plt.figure(figsize=(6, 4))  # Tamaño del gráfico\n",
        "    sns.boxplot(x=df[col])\n",
        "    plt.title(f'{col} - Datos atípicos', fontsize=14)\n",
        "    plt.xlabel(col)\n",
        "    plt.savefig(f\"{col}_boxplot.png\")  # Guarda la imagen\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_YvWVI9E7w7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Convertir variables categóricas a numéricas usando codificación ordinal\n",
        "df_encoded = df_imputado.copy()\n",
        "\n",
        "categorical_columns = [\n",
        "    \"State\", \"Sex\", \"GeneralHealth\", \"LastCheckupTime\", \"PhysicalActivities\",\n",
        "    \"RemovedTeeth\", \"HadHeartAttack\", \"HadAngina\", \"HadStroke\", \"HadAsthma\",\n",
        "    \"HadSkinCancer\", \"HadCOPD\", \"HadDepressiveDisorder\", \"HadKidneyDisease\",\n",
        "    \"HadArthritis\", \"HadDiabetes\", \"DeafOrHardOfHearing\", \"BlindOrVisionDifficulty\",\n",
        "    \"DifficultyConcentrating\", \"DifficultyWalking\", \"DifficultyDressingBathing\",\n",
        "    \"DifficultyErrands\", \"SmokerStatus\", \"ECigaretteUsage\", \"ChestScan\",\n",
        "    \"RaceEthnicityCategory\", \"AgeCategory\", \"AlcoholDrinkers\", \"HIVTesting\",\n",
        "    \"FluVaxLast12\", \"PneumoVaxEver\", \"TetanusLast10Tdap\", \"HighRiskLastYear\", \"CovidPos\"\n",
        "]\n",
        "\n",
        "for col in categorical_columns:\n",
        "    df_encoded[col] = df_encoded[col].astype(\"category\").cat.codes  # Asigna códigos numéricos a las categorías\n",
        "\n",
        "# Calcular la matriz de correlación\n",
        "corr_matrix = df_encoded.corr()\n",
        "\n",
        "# Crear el heatmap con valores numéricos visibles y mayor tamaño\n",
        "plt.figure(figsize=(20, 15))\n",
        "sns.heatmap(\n",
        "    corr_matrix,\n",
        "    cmap=\"coolwarm\",\n",
        "    annot=True,\n",
        "    fmt=\".2f\",\n",
        "    linewidths=0.5,\n",
        "    annot_kws={\"size\": 8}  # Reduce el tamaño de los números\n",
        ")\n",
        "\n",
        "# Configurar etiquetas\n",
        "plt.xticks(rotation=90)  # Rotar etiquetas del eje X para mejor visualización\n",
        "plt.yticks(rotation=0)   # Mantener las etiquetas del eje Y horizontales\n",
        "plt.title(\"Mapa de calor de correlaciones entre variables\", fontsize=20)\n",
        "\n",
        "# Mostrar el heatmap\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Z05KinyD1cul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUz4MtgDpYh0"
      },
      "outputs": [],
      "source": [
        "# Crear el gráfico de conteo con df_imputado\n",
        "ax = sns.countplot(data=df_imputado, x='HadHeartAttack', hue='Sex')\n",
        "\n",
        "# Función para personalizar el gráfico\n",
        "def customize_plot(ax, title, xlabel, ylabel, width, height):\n",
        "    ax.set_title(title, fontsize=16)\n",
        "    ax.set_xlabel(xlabel, fontsize=14)\n",
        "    ax.set_ylabel(ylabel, fontsize=14)\n",
        "    plt.xticks(fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "    plt.gcf().set_size_inches(width, height)  # Ajustar tamaño de la figura\n",
        "\n",
        "# Aplicar la personalización\n",
        "customize_plot(ax, \"Genders and Heart Attack\", \"Had Heart Attack\", \"Individuals\", 12, 10)\n",
        "\n",
        "# Guardar el gráfico en un archivo\n",
        "scatter_fig = ax.get_figure()\n",
        "scatter_fig.savefig('genderHeartAttack.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "# Mostrar el gráfico\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5q6SitJqGnN"
      },
      "outputs": [],
      "source": [
        "# Asegúrate de que 'AgeCategory' sea una columna categórica\n",
        "df_imputado['AgeCategory'] = df_imputado['AgeCategory'].astype('category')\n",
        "\n",
        "# Crear el gráfico de barras\n",
        "ax = sns.countplot(data=df_imputado, x='AgeCategory', hue='HadHeartAttack')\n",
        "\n",
        "# Rotar etiquetas del eje X para mejorar legibilidad\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
        "\n",
        "# Personalizar el gráfico\n",
        "customize_plot(ax, \"Heart Attacks and Age Groups\", \"Had Heart Attack\", \"Individuals\", 12, 10)\n",
        "\n",
        "# Añadir etiquetas a las barras\n",
        "for c in ax.containers:\n",
        "    ax.bar_label(c)\n",
        "\n",
        "# Guardar el gráfico\n",
        "scatter_fig = ax.get_figure()\n",
        "scatter_fig.savefig('countplotByAge.png')\n",
        "\n",
        "# Mostrar el gráfico\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SrSsNbfvxUG"
      },
      "source": [
        "# **6) Árbol RandomForestClassifier**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7uB5vkHLTml"
      },
      "outputs": [],
      "source": [
        "df_imputado.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zikYlQxmNjzT"
      },
      "outputs": [],
      "source": [
        "for col in df: # Verificar si hay una columna o atributo que no aporte, ejemplo si tiene 1 subnivel, no nos sirve\n",
        "  print(f\"Columna {col}: {df_imputado[col].nunique() } subniveles \")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. Conteo de clases\n",
        "class_counts = df_imputado['HadHeartAttack'].value_counts()\n",
        "\n",
        "# 2. Gráfico de barras\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=class_counts.index, y=class_counts.values, palette=\"viridis\")\n",
        "plt.title(\"Distribución de Ataques Cardíacos (Conteo)\")\n",
        "plt.xlabel(\"HadHeartAttack\")\n",
        "plt.ylabel(\"Número de casos\")\n",
        "plt.show()\n",
        "\n",
        "# 3. Gráfico de pastel (porcentajes)\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.pie(class_counts, labels=class_counts.index, autopct='%1.1f%%', colors=['#ff9999','#66b3ff'])\n",
        "plt.title(\"Proporción de Ataques Cardíacos\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TByndW8y49eY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNQ_PSiiNvFw"
      },
      "outputs": [],
      "source": [
        "# Obtener valores únicos de todas las columnas en df_imputado\n",
        "for col in df_imputado.columns:\n",
        "    print(f\"\\n Valores únicos en la columna {col}: {df_imputado[col].unique()}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 1. Preparación de Datos\n",
        "# --------------------------------------------------\n",
        "target_column = \"HadHeartAttack\"\n",
        "\n",
        "# Convertir la variable objetivo a categórica directamente\n",
        "df_imputado[target_column] = df_imputado[target_column].astype('category')\n",
        "\n",
        "# Definir variables predictoras (X) y la variable objetivo (y)\n",
        "X = df_imputado.drop(columns=[target_column])\n",
        "y = df_imputado[target_column]  # Mantenemos como categoría\n",
        "\n",
        "# Identificar columnas categóricas en X\n",
        "categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
        "X[categorical_columns] = X[categorical_columns].astype('category')\n",
        "\n",
        "# 2. División de Datos\n",
        "# --------------------------------------------------\n",
        "# Dividir manteniendo las categorías originales\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=87, stratify=y\n",
        ")\n",
        "\n",
        "# 3. Configuración y Entrenamiento del Modelo\n",
        "# --------------------------------------------------\n",
        "lgb_model = lgb.LGBMClassifier(\n",
        "    boosting_type='goss',\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.01,\n",
        "    max_depth=-1,\n",
        "    num_leaves=63,\n",
        "    min_data_in_leaf=30,\n",
        "    feature_fraction=0.8,\n",
        "    lambda_l1=0.1,\n",
        "    lambda_l2=0.1,\n",
        "    # class_weight='balanced',\n",
        "    class_weight={'No': 1, 'Yes': 5},\n",
        "    random_state=42,\n",
        "    objective='binary',  # Asegurar que es para clasificación binaria\n",
        "    metric='binary_logloss'\n",
        ")\n",
        "\n",
        "# Entrenamiento con early stopping (una sola vez)\n",
        "lgb_model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_test, y_test)],\n",
        "    eval_metric='binary_logloss',\n",
        "    categorical_feature=categorical_columns,\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=50)]\n",
        ")\n",
        "\n",
        "# 4. Predicción y Evaluación\n",
        "# --------------------------------------------------\n",
        "# Realizar predicciones (ya están en las categorías originales)\n",
        "y_pred = lgb_model.predict(X_test)\n",
        "\n",
        "# Calcular métricas directamente con las categorías\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, pos_label='Yes')\n",
        "recall = recall_score(y_test, y_pred, pos_label='Yes')\n",
        "f1 = f1_score(y_test, y_pred, pos_label='Yes')\n",
        "\n",
        "# Imprimir métricas\n",
        "print(\"\\nMétricas de Evaluación:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "# 5. Visualización\n",
        "# --------------------------------------------------\n",
        "# Matriz de confusión\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=['No Heart Attack', 'Heart Attack'],\n",
        "            yticklabels=['No Heart Attack', 'Heart Attack'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Matriz de Confusión')\n",
        "plt.show()\n",
        "\n",
        "# Curva Precision-Recall (Opcional)\n",
        "\n",
        "y_probs = lgb_model.predict_proba(X_test)[:, 1]\n",
        "precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_probs, pos_label='Yes')\n",
        "ap_score = average_precision_score(y_test, y_probs, pos_label='Yes')\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(recall_curve, precision_curve, label=f'AP Score: {ap_score:.2f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Curva Precision-Recall')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bBrL7HPiTtQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Mal Rendimiento**\n",
        "Como el rendimiento no es tan bueno, ajustaremos el dataset conservando solo las columnas relevantes, ya que las demás están afectando negativamente el rendimiento"
      ],
      "metadata": {
        "id": "XFZ11SntR1Qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columnas_a_eliminar = [\n",
        "    # variable objectivo. así que no debe estar en x para predecirla así misma\n",
        "    'HadHeartAttack',\n",
        "    'State',  # Ubicación geográfica no es un factor médico directo\n",
        "    'DeafOrHardOfHearing',  # Problemas auditivos no relacionados\n",
        "    'BlindOrVisionDifficulty',  # Problemas visuales no relacionados\n",
        "    'HIVTesting',  # No directamente relacionado con salud cardíaca\n",
        "    'RemovedTeeth',  # Salud dental no es predictor cardíaco\n",
        "    'ChestScan',  # Es un examen, no un factor de riesgo\n",
        "    'FluVaxLast12',  # Vacuna de gripe no es relevante\n",
        "    'PneumoVaxEver',  # Vacuna neumococo no es relevante\n",
        "    'CovidPos',  # Muy reciente para tener datos concluyentes\n",
        "    'HeightInMeters',  # Mejor usar BMI que combina altura/peso\n",
        "    'WeightInKilograms',  # Mejor usar BMI\n",
        "    'DifficultyConcentrating',  # Síntoma muy genérico\n",
        "    'DifficultyDressingBathing',  # Movilidad no específica cardíaca\n",
        "    'DifficultyErrands'  # Movilidad no específica cardíaca\n",
        "]\n",
        "\n",
        "\n",
        "variables_clave = [\n",
        "\n",
        "\n",
        "    # Factores demográficos básicos\n",
        "    'Sex',\n",
        "    'AgeCategory',\n",
        "\n",
        "    # Salud general\n",
        "    'GeneralHealth',\n",
        "    'PhysicalHealthDays',\n",
        "    'MentalHealthDays',\n",
        "    'BMI',\n",
        "\n",
        "    # Factores de riesgo cardiovascular\n",
        "    'HadAngina',\n",
        "    'HadStroke',\n",
        "    'HadAsthma',\n",
        "    'HadCOPD',\n",
        "    'HadDiabetes',\n",
        "    'HadKidneyDisease',\n",
        "    'HadArthritis',\n",
        "\n",
        "    # Hábitos de vida\n",
        "    'SmokerStatus',\n",
        "    'ECigaretteUsage',\n",
        "    'AlcoholDrinkers',\n",
        "    'PhysicalActivities',\n",
        "    'SleepHours',\n",
        "\n",
        "    # Comorbilidades relevantes\n",
        "    'HadDepressiveDisorder',\n",
        "    'HadSkinCancer',  # Algunos estudios muestran correlación\n",
        "\n",
        "    # Exámenes médicos\n",
        "    'LastCheckupTime',\n",
        "    'HighRiskLastYear',\n",
        "\n",
        "    # Dificultades físicas relacionadas\n",
        "    'DifficultyWalking'  # Puede indicar problemas circulatorios\n",
        "]"
      ],
      "metadata": {
        "id": "MRR0pBB-R0vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.unique()"
      ],
      "metadata": {
        "id": "OD0TlV04vxm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.unique()"
      ],
      "metadata": {
        "id": "FY3345u5vx4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "mS4ql40O1R00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.info()"
      ],
      "metadata": {
        "id": "AbaVn2-F10mV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 1. Preparación de Datos\n",
        "# --------------------------------------------------\n",
        "target_column = \"HadHeartAttack\"\n",
        "\n",
        "# Convertir la variable objetivo a categórica directamente\n",
        "df_imputado[target_column] = df_imputado[target_column].astype('category')\n",
        "\n",
        "# Definir variables predictoras (X) y la variable objetivo (y)\n",
        "X = df_imputado.drop(columns=columnas_a_eliminar)  # Elimina todo de una vez\n",
        "y = df_imputado[target_column]  # Mantenemos como categoría\n",
        "\n",
        "# Identificar columnas categóricas en X\n",
        "categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
        "X[categorical_columns] = X[categorical_columns].astype('category')\n",
        "\n",
        "# 2. División de Datos\n",
        "# --------------------------------------------------\n",
        "# Dividir manteniendo las categorías originales\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=87, stratify=y\n",
        ")\n",
        "\n",
        "# 3. Configuración y Entrenamiento del Modelo\n",
        "# --------------------------------------------------\n",
        "lgb_model = lgb.LGBMClassifier(\n",
        "    boosting_type='goss',  # Gradient-based One-Side Sampling\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=15,\n",
        "    num_leaves=31,\n",
        "    min_child_samples=50,\n",
        "    reg_alpha=0.5,\n",
        "    reg_lambda=0.5,\n",
        "    class_weight={'No': 1, 'Yes': 5},\n",
        "    random_state=42,\n",
        "    objective='binary',\n",
        "    metric='aucpr',\n",
        "    n_jobs=-1,\n",
        "    importance_type='gain',\n",
        "    min_data_in_leaf=100,\n",
        "    cat_smooth=20,\n",
        "    extra_trees=True\n",
        ")\n",
        "\n",
        "# Entrenamiento con early stopping (una sola vez)\n",
        "lgb_model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_test, y_test)],\n",
        "    eval_metric='binary_logloss',\n",
        "    categorical_feature=categorical_columns,\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=10)]\n",
        ")\n",
        "\n",
        "# 4. Predicción y Evaluación\n",
        "# --------------------------------------------------\n",
        "# Realizar predicciones (ya están en las categorías originales)\n",
        "y_pred = lgb_model.predict(X_test)\n",
        "\n",
        "# Calcular métricas directamente con las categorías\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, pos_label='Yes')\n",
        "recall = recall_score(y_test, y_pred, pos_label='Yes')\n",
        "f1 = f1_score(y_test, y_pred, pos_label='Yes')\n",
        "\n",
        "# Imprimir métricas\n",
        "print(\"\\nMétricas de Evaluación:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "# 5. Visualización\n",
        "# --------------------------------------------------\n",
        "# Matriz de confusión\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=['No Heart Attack', 'Heart Attack'],\n",
        "            yticklabels=['No Heart Attack', 'Heart Attack'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Matriz de Confusión')\n",
        "plt.show()\n",
        "\n",
        "# Curva Precision-Recall (Opcional)\n",
        "\n",
        "y_probs = lgb_model.predict_proba(X_test)[:, 1]\n",
        "precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_probs, pos_label='Yes')\n",
        "ap_score = average_precision_score(y_test, y_probs, pos_label='Yes')\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(recall_curve, precision_curve, label=f'AP Score: {ap_score:.2f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Curva Precision-Recall')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Izrl70HJ6kTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.info()"
      ],
      "metadata": {
        "id": "r2sMs7ijRcMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[[\"AgeCategory\"]]"
      ],
      "metadata": {
        "id": "RvpihKcDSYTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Randont Forest Pero con las Filas nulas eliminadas \"HadHearAttack = No\" e imputados en HadHearAttack =SI\"**"
      ],
      "metadata": {
        "id": "--Dq3DiByL_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 1. Crear copia del dataframe original\n",
        "df= df.copy()\n",
        "\n",
        "# 2. Separar los casos positivos (Yes) y negativos (No)\n",
        "positivos = df[df['HadHeartAttack'] == 'Yes']\n",
        "negativos = df[df['HadHeartAttack'] == 'No']\n",
        "\n",
        "# 3. Eliminar filas con valores nulos SOLO en los negativos (No)\n",
        "negativos_limpios = negativos.dropna()\n",
        "\n",
        "# 4. Combinar los positivos completos con los negativos limpios\n",
        "df_Prueba = pd.concat([positivos, negativos_limpios], axis=0)\n",
        "\n",
        "# 5. Imputación simple para las variables numéricas y categóricas restantes\n",
        "# Seleccionar columnas numéricas y categóricas\n",
        "numeric_cols = df_Prueba.select_dtypes(include=['float64', 'int64']).columns\n",
        "categorical_cols = df_Prueba.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# Imputar numéricos con la mediana (menos sensible a outliers)\n",
        "imputer_num = SimpleImputer(strategy='median')\n",
        "df_Prueba[numeric_cols] = imputer_num.fit_transform(df_Prueba[numeric_cols])\n",
        "\n",
        "# Imputar categóricas con la moda (valor más frecuente)\n",
        "imputer_cat = SimpleImputer(strategy='most_frequent')\n",
        "df_Prueba[categorical_cols] = imputer_cat.fit_transform(df_Prueba[categorical_cols])\n",
        "\n",
        "# Verificación final\n",
        "print(f\"Registros originales: {len(df_Prueba)}\")\n",
        "print(f\"Registros después del procesamiento: {len(df_Prueba)}\")\n",
        "print(f\"Distribución de clases:\\n{df_Prueba['HadHeartAttack'].value_counts(normalize=True)}\")"
      ],
      "metadata": {
        "id": "uzeueeOr0YVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 1. Conteo de clases\n",
        "class_counts = df_Prueba['HadHeartAttack'].value_counts()\n",
        "\n",
        "# 2. Gráfico de barras\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=class_counts.index, y=class_counts.values, palette=\"viridis\")\n",
        "plt.title(\"Distribución de Ataques Cardíacos (Conteo)\")\n",
        "plt.xlabel(\"HadHeartAttack\")\n",
        "plt.ylabel(\"Número de casos\")\n",
        "plt.show()\n",
        "\n",
        "# 3. Gráfico de pastel (porcentajes)\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.pie(class_counts, labels=class_counts.index, autopct='%1.1f%%', colors=['#ff9999','#66b3ff'])\n",
        "plt.title(\"Proporción de Ataques Cardíacos\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XHRrH8XmyIr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 1. Preparación de Datos\n",
        "# --------------------------------------------------\n",
        "target_column = \"HadHeartAttack\"\n",
        "\n",
        "# Convertir la variable objetivo a categórica directamente\n",
        "df_Prueba[target_column] = df_Prueba[target_column].astype('category')\n",
        "\n",
        "# Definir variables predictoras (X) y la variable objetivo (y)\n",
        "X = df_Prueba.drop(columns=columnas_a_eliminar)  # Elimina todo de una vez\n",
        "y = df_Prueba[target_column]  # Mantenemos como categoría\n",
        "\n",
        "# Identificar columnas categóricas en X\n",
        "categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
        "X[categorical_columns] = X[categorical_columns].astype('category')\n",
        "\n",
        "# 2. División de Datos\n",
        "# --------------------------------------------------\n",
        "# Dividir manteniendo las categorías originales\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=87, stratify=y\n",
        ")\n",
        "\n",
        "# 3. Configuración y Entrenamiento del Modelo\n",
        "# --------------------------------------------------\n",
        "lgb_model = lgb.LGBMClassifier(\n",
        "    boosting_type='goss',  # Gradient-based One-Side Sampling\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=15,\n",
        "    num_leaves=31,\n",
        "    min_child_samples=50,\n",
        "    reg_alpha=0.5,\n",
        "    reg_lambda=0.5,\n",
        "    class_weight={'No': 1, 'Yes': 5},\n",
        "    random_state=42,\n",
        "    objective='binary',\n",
        "    metric='aucpr',\n",
        "    n_jobs=-1,\n",
        "    importance_type='gain',\n",
        "    min_data_in_leaf=100,\n",
        "    cat_smooth=20,\n",
        "    extra_trees=True\n",
        ")\n",
        "\n",
        "# Entrenamiento con early stopping (una sola vez)\n",
        "lgb_model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_test, y_test)],\n",
        "    eval_metric='binary_logloss',\n",
        "    categorical_feature=categorical_columns,\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=10)]\n",
        ")\n",
        "\n",
        "# 4. Predicción y Evaluación\n",
        "# --------------------------------------------------\n",
        "# Realizar predicciones (ya están en las categorías originales)\n",
        "y_pred = lgb_model.predict(X_test)\n",
        "\n",
        "# Calcular métricas directamente con las categorías\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, pos_label='Yes')\n",
        "recall = recall_score(y_test, y_pred, pos_label='Yes')\n",
        "f1 = f1_score(y_test, y_pred, pos_label='Yes')\n",
        "\n",
        "# Imprimir métricas\n",
        "print(\"\\nMétricas de Evaluación:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "# 5. Visualización\n",
        "# --------------------------------------------------\n",
        "# Matriz de confusión\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=['No Heart Attack', 'Heart Attack'],\n",
        "            yticklabels=['No Heart Attack', 'Heart Attack'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Matriz de Confusión')\n",
        "plt.show()\n",
        "\n",
        "# Curva Precision-Recall (Opcional)\n",
        "\n",
        "y_probs = lgb_model.predict_proba(X_test)[:, 1]\n",
        "precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_probs, pos_label='Yes')\n",
        "ap_score = average_precision_score(y_test, y_probs, pos_label='Yes')\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(recall_curve, precision_curve, label=f'AP Score: {ap_score:.2f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Curva Precision-Recall')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NprVgey318UL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFj0zbvELlmBGXLf52sJxP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}