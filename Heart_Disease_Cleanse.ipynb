{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DiegoLuis62/Ciencias-de-datos---PF/blob/main/Heart_Disease_Cleanse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpLXvYrfxLrH"
      },
      "source": [
        "# **1. Carga y exploraci贸n inicial del dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "7tQt43dEhg-T"
      },
      "outputs": [],
      "source": [
        "# Bibliotecas est谩ndar\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Manipulaci贸n de datos\n",
        "import pandas as pd\n",
        "\n",
        "# Visualizaci贸n\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    precision_recall_curve,\n",
        "    average_precision_score\n",
        ")\n",
        "from sklearn.impute import SimpleImputer\n",
        "import lightgbm as lgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTa_8GeYhvLT"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "dataset_path = \"/content/drive/My Drive/10 semestre/PF-Dataset\"\n",
        "df = pd.read_csv(dataset_path + \"/heart_2022_with_nans.csv\")\n",
        "# Link del dataset : https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raj42tp_lk4O"
      },
      "outputs": [],
      "source": [
        "print(df.shape)\n",
        "print(\"\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7dg19PPmj1F"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bBRHPktwJuv"
      },
      "source": [
        "# **2. An谩lisis de valores nulos: cantidad, porcentaje y tipo de datos**\n",
        "\n",
        "En este an谩lisis, se identifican las columnas que contienen valores nulos dentro del dataset. Se calcula la cantidad y el porcentaje de datos faltantes en cada variable, adem谩s de determinar su tipo de dato (num茅rico o categ贸rico). Esto permite evaluar el impacto de los valores ausentes y definir estrategias adecuadas para la limpieza y el tratamiento de los datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WA1cEZiiokl-"
      },
      "outputs": [],
      "source": [
        "missing_values = df.isnull().sum()\n",
        "\n",
        "# Filtrar solo las columnas con NaN\n",
        "missing_columns = missing_values[missing_values > 0]\n",
        "\n",
        "# Obtener los tipos de datos de esas columnas\n",
        "dtypes = df.dtypes[missing_columns.index]\n",
        "\n",
        "# Crear un DataFrame con la informaci贸n\n",
        "missing_info = pd.DataFrame({\n",
        "    \"Missing Values\": missing_columns,\n",
        "    \"Percentage\": (missing_columns / len(df)) * 100,\n",
        "    \"Data Type\": dtypes\n",
        "})\n",
        "\n",
        "\n",
        "missing_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DB1XmIPEW4nV"
      },
      "outputs": [],
      "source": [
        "for col in df: # Verificar si hay una columna o atributo que no aporte, ejemplo si tiene 1 subnivel, no nos sirve\n",
        "  print(f\"Columna {col}: {df[col].nunique() } subniveles \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiWCHnycktOj"
      },
      "outputs": [],
      "source": [
        "# Verificar cantidad de filas duplicadas en el dataset original\n",
        "duplicados = df.duplicated().sum()\n",
        "\n",
        "print(f\"N煤mero de filas duplicadas en el dataset original: {duplicados}\")\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Acabo de eliminar 157 datos duplicados del dataset original para evitar tener que eliminarlos uno por uno al copiar los dem谩s datasets."
      ],
      "metadata": {
        "id": "jOnWzxeB6lcX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1p0mQPl1BiM0"
      },
      "source": [
        "#**3) Limpieza de los Datasets**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1-9kHCFDKHz"
      },
      "source": [
        "#Limpieza de Dataset por Eliminaci贸n\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yI0bYhDvwFkk"
      },
      "outputs": [],
      "source": [
        "df_cleaned = df.dropna()  # Elimina filas con valores nulos\n",
        "print(f\"Filas originales: {df.shape[0]}\")\n",
        "print(f\"Filas despu茅s de limpieza: {df_cleaned.shape[0]}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-PC8UyrwsVE"
      },
      "source": [
        "#Limpieza de Dataset Usando Moda, Mediana"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2Ck33hEy7XV"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "df_imputado = df.copy()\n",
        "\n",
        "# Identificar variables categ贸ricas y num茅ricas\n",
        "categorical_vars = df_imputado.select_dtypes(include=['object']).columns\n",
        "binary_vars = [var for var in categorical_vars if df_imputado[var].nunique() == 2]  # Variables binarias\n",
        "num_vars = df_imputado.select_dtypes(include=['float64']).columns\n",
        "\n",
        "# Imputaci贸n con moda para categ贸ricas y binarias\n",
        "imputer_moda = SimpleImputer(strategy=\"most_frequent\")\n",
        "df_imputado[categorical_vars] = imputer_moda.fit_transform(df_imputado[categorical_vars])\n",
        "\n",
        "# Imputaci贸n con mediana para variables num茅ricas\n",
        "imputer_mediana = SimpleImputer(strategy=\"median\")\n",
        "df_imputado[num_vars] = imputer_mediana.fit_transform(df_imputado[num_vars])\n",
        "\n",
        "# Guardar dataset imputado\n",
        "df_imputado.to_csv(\"dataset_imputado.csv\", index=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Filas originales: {df.shape[0]}\")\n",
        "print(f\"Filas despu茅s de imputar: {df_imputado.shape[0]}\")"
      ],
      "metadata": {
        "id": "R17hzzKh4-fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4) Comparaci贸n de columnas num茅ricas del dataset original, eliminaci贸n de datos nulos e imputaci贸n.**"
      ],
      "metadata": {
        "id": "REqrJahj5UQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset con datos Originales**"
      ],
      "metadata": {
        "id": "KWZso2fg6H9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.describe()\n"
      ],
      "metadata": {
        "id": "ww8YfmXA5Jr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset con datos nulos eliminados**"
      ],
      "metadata": {
        "id": "aDQGC7JA5235"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8HNBfqwYuY2"
      },
      "outputs": [],
      "source": [
        "df_cleaned.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset con datos Imputados**"
      ],
      "metadata": {
        "id": "DQCLocBr6MCl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "883g4M82zj0X"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_imputado.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfcTILek0y8Y"
      },
      "source": [
        "# **An谩lisis Comparativo de M茅todos para el Manejo de Datos Faltantes**\n",
        "\n",
        "En el presente an谩lisis, se compararon dos estrategias para tratar los datos faltantes en el dataset: la eliminaci贸n de registros con valores nulos y la imputaci贸n mediante la moda (para datos categ贸ricos) y la mediana (para datos num茅ricos). A continuaci贸n, se presentan los hallazgos clave.\n",
        "\n",
        "# 1锔 Eliminaci贸n de Datos Faltantes\n",
        "\n",
        "Se redujo significativamente la cantidad de datos, pasando de ~434,205 registros a 246,022 en algunas columnas, representando una p茅rdida aproximada del 43.3% de la informaci贸n.\n",
        "\n",
        "La media de variables clave, como el ndice de Masa Corporal (BMI), pas贸 de 28.53 en el dataset original a 28.67 tras la eliminaci贸n de datos, lo que sugiere que los valores eliminados ten铆an una distribuci贸n diferente al conjunto de datos restante.\n",
        "\n",
        "Se observ贸 una disminuci贸n en la dispersi贸n de los datos (medida a trav茅s de la desviaci贸n est谩ndar), lo que indica una posible p茅rdida de variabilidad en la muestra.\n",
        "\n",
        "# 2锔 Imputaci贸n con Moda y Mediana\n",
        "\n",
        "Se conservaron 445,132 registros, manteniendo la totalidad de la muestra sin p茅rdida de informaci贸n.\n",
        "\n",
        "La media del BMI fue 28.41, mucho m谩s cercana al valor original (28.53) en comparaci贸n con la eliminaci贸n de datos.\n",
        "\n",
        "La desviaci贸n est谩ndar disminuy贸 ligeramente en comparaci贸n con el dataset original, pero sin alterar significativamente la variabilidad inherente a los datos.\n",
        "\n",
        "#  Conclusi贸n\n",
        "\n",
        "El an谩lisis comparativo evidencia que la eliminaci贸n de datos genera una p茅rdida considerable de informaci贸n y puede introducir sesgos en la muestra al alterar las estad铆sticas descriptivas de las variables. En contraste, la imputaci贸n con moda y mediana preserva la estructura del dataset y mantiene los valores estad铆sticos m谩s cercanos a los originales.\n",
        "\n",
        "Por lo tanto, para evitar la p茅rdida de informaci贸n valiosa y minimizar el sesgo en futuros an谩lisis, se recomienda la imputaci贸n de valores faltantes en lugar de la eliminaci贸n de registros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbBj0wv-mkLg"
      },
      "source": [
        "# **5) An谩lisis de datos**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Suponiendo que tienes tu DataFrame llamado 'df'\n",
        "numerical_columns = [\n",
        "    \"PhysicalHealthDays\", \"MentalHealthDays\", \"SleepHours\",\n",
        "    \"HeightInMeters\", \"WeightInKilograms\", \"BMI\"\n",
        "]\n",
        "\n",
        "# Crear boxplots para cada columna num茅rica\n",
        "for col in numerical_columns:\n",
        "    plt.figure(figsize=(6, 4))  # Tama帽o del gr谩fico\n",
        "    sns.boxplot(x=df[col])\n",
        "    plt.title(f'{col} - Datos at铆picos', fontsize=14)\n",
        "    plt.xlabel(col)\n",
        "    plt.savefig(f\"{col}_boxplot.png\")  # Guarda la imagen\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_YvWVI9E7w7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Convertir variables categ贸ricas a num茅ricas usando codificaci贸n ordinal\n",
        "df_encoded = df_imputado.copy()\n",
        "\n",
        "categorical_columns = [\n",
        "    \"State\", \"Sex\", \"GeneralHealth\", \"LastCheckupTime\", \"PhysicalActivities\",\n",
        "    \"RemovedTeeth\", \"HadHeartAttack\", \"HadAngina\", \"HadStroke\", \"HadAsthma\",\n",
        "    \"HadSkinCancer\", \"HadCOPD\", \"HadDepressiveDisorder\", \"HadKidneyDisease\",\n",
        "    \"HadArthritis\", \"HadDiabetes\", \"DeafOrHardOfHearing\", \"BlindOrVisionDifficulty\",\n",
        "    \"DifficultyConcentrating\", \"DifficultyWalking\", \"DifficultyDressingBathing\",\n",
        "    \"DifficultyErrands\", \"SmokerStatus\", \"ECigaretteUsage\", \"ChestScan\",\n",
        "    \"RaceEthnicityCategory\", \"AgeCategory\", \"AlcoholDrinkers\", \"HIVTesting\",\n",
        "    \"FluVaxLast12\", \"PneumoVaxEver\", \"TetanusLast10Tdap\", \"HighRiskLastYear\", \"CovidPos\"\n",
        "]\n",
        "\n",
        "for col in categorical_columns:\n",
        "    df_encoded[col] = df_encoded[col].astype(\"category\").cat.codes  # Asigna c贸digos num茅ricos a las categor铆as\n",
        "\n",
        "# Calcular la matriz de correlaci贸n\n",
        "corr_matrix = df_encoded.corr()\n",
        "\n",
        "# Crear el heatmap con valores num茅ricos visibles y mayor tama帽o\n",
        "plt.figure(figsize=(20, 15))\n",
        "sns.heatmap(\n",
        "    corr_matrix,\n",
        "    cmap=\"coolwarm\",\n",
        "    annot=True,\n",
        "    fmt=\".2f\",\n",
        "    linewidths=0.5,\n",
        "    annot_kws={\"size\": 8}  # Reduce el tama帽o de los n煤meros\n",
        ")\n",
        "\n",
        "# Configurar etiquetas\n",
        "plt.xticks(rotation=90)  # Rotar etiquetas del eje X para mejor visualizaci贸n\n",
        "plt.yticks(rotation=0)   # Mantener las etiquetas del eje Y horizontales\n",
        "plt.title(\"Mapa de calor de correlaciones entre variables\", fontsize=20)\n",
        "\n",
        "# Mostrar el heatmap\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Z05KinyD1cul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUz4MtgDpYh0"
      },
      "outputs": [],
      "source": [
        "# Crear el gr谩fico de conteo con df_imputado\n",
        "ax = sns.countplot(data=df_imputado, x='HadHeartAttack', hue='Sex')\n",
        "\n",
        "# Funci贸n para personalizar el gr谩fico\n",
        "def customize_plot(ax, title, xlabel, ylabel, width, height):\n",
        "    ax.set_title(title, fontsize=16)\n",
        "    ax.set_xlabel(xlabel, fontsize=14)\n",
        "    ax.set_ylabel(ylabel, fontsize=14)\n",
        "    plt.xticks(fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "    plt.gcf().set_size_inches(width, height)  # Ajustar tama帽o de la figura\n",
        "\n",
        "# Aplicar la personalizaci贸n\n",
        "customize_plot(ax, \"Genders and Heart Attack\", \"Had Heart Attack\", \"Individuals\", 12, 10)\n",
        "\n",
        "# Guardar el gr谩fico en un archivo\n",
        "scatter_fig = ax.get_figure()\n",
        "scatter_fig.savefig('genderHeartAttack.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "# Mostrar el gr谩fico\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5q6SitJqGnN"
      },
      "outputs": [],
      "source": [
        "# Aseg煤rate de que 'AgeCategory' sea una columna categ贸rica\n",
        "df_imputado['AgeCategory'] = df_imputado['AgeCategory'].astype('category')\n",
        "\n",
        "# Crear el gr谩fico de barras\n",
        "ax = sns.countplot(data=df_imputado, x='AgeCategory', hue='HadHeartAttack')\n",
        "\n",
        "# Rotar etiquetas del eje X para mejorar legibilidad\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
        "\n",
        "# Personalizar el gr谩fico\n",
        "customize_plot(ax, \"Heart Attacks and Age Groups\", \"Had Heart Attack\", \"Individuals\", 12, 10)\n",
        "\n",
        "# A帽adir etiquetas a las barras\n",
        "for c in ax.containers:\n",
        "    ax.bar_label(c)\n",
        "\n",
        "# Guardar el gr谩fico\n",
        "scatter_fig = ax.get_figure()\n",
        "scatter_fig.savefig('countplotByAge.png')\n",
        "\n",
        "# Mostrar el gr谩fico\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SrSsNbfvxUG"
      },
      "source": [
        "# **6) rbol RandomForestClassifier**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7uB5vkHLTml"
      },
      "outputs": [],
      "source": [
        "df_imputado.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zikYlQxmNjzT"
      },
      "outputs": [],
      "source": [
        "for col in df: # Verificar si hay una columna o atributo que no aporte, ejemplo si tiene 1 subnivel, no nos sirve\n",
        "  print(f\"Columna {col}: {df_imputado[col].nunique() } subniveles \")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. Conteo de clases\n",
        "class_counts = df_imputado['HadHeartAttack'].value_counts()\n",
        "\n",
        "# 2. Gr谩fico de barras\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=class_counts.index, y=class_counts.values, palette=\"viridis\")\n",
        "plt.title(\"Distribuci贸n de Ataques Card铆acos (Conteo)\")\n",
        "plt.xlabel(\"HadHeartAttack\")\n",
        "plt.ylabel(\"N煤mero de casos\")\n",
        "plt.show()\n",
        "\n",
        "# 3. Gr谩fico de pastel (porcentajes)\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.pie(class_counts, labels=class_counts.index, autopct='%1.1f%%', colors=['#ff9999','#66b3ff'])\n",
        "plt.title(\"Proporci贸n de Ataques Card铆acos\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TByndW8y49eY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNQ_PSiiNvFw"
      },
      "outputs": [],
      "source": [
        "# Obtener valores 煤nicos de todas las columnas en df_imputado\n",
        "for col in df_imputado.columns:\n",
        "    print(f\"\\n Valores 煤nicos en la columna {col}: {df_imputado[col].unique()}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 1. Preparaci贸n de Datos\n",
        "# --------------------------------------------------\n",
        "target_column = \"HadHeartAttack\"\n",
        "\n",
        "# Convertir la variable objetivo a categ贸rica directamente\n",
        "df_imputado[target_column] = df_imputado[target_column].astype('category')\n",
        "\n",
        "# Definir variables predictoras (X) y la variable objetivo (y)\n",
        "X = df_imputado.drop(columns=[target_column])\n",
        "y = df_imputado[target_column]  # Mantenemos como categor铆a\n",
        "\n",
        "# Identificar columnas categ贸ricas en X\n",
        "categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
        "X[categorical_columns] = X[categorical_columns].astype('category')\n",
        "\n",
        "# 2. Divisi贸n de Datos\n",
        "# --------------------------------------------------\n",
        "# Dividir manteniendo las categor铆as originales\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=87, stratify=y\n",
        ")\n",
        "\n",
        "# 3. Configuraci贸n y Entrenamiento del Modelo\n",
        "# --------------------------------------------------\n",
        "lgb_model = lgb.LGBMClassifier(\n",
        "    boosting_type='goss',\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.01,\n",
        "    max_depth=-1,\n",
        "    num_leaves=63,\n",
        "    min_data_in_leaf=30,\n",
        "    feature_fraction=0.8,\n",
        "    lambda_l1=0.1,\n",
        "    lambda_l2=0.1,\n",
        "    # class_weight='balanced',\n",
        "    class_weight={'No': 1, 'Yes': 5},\n",
        "    random_state=42,\n",
        "    objective='binary',  # Asegurar que es para clasificaci贸n binaria\n",
        "    metric='binary_logloss'\n",
        ")\n",
        "\n",
        "# Entrenamiento con early stopping (una sola vez)\n",
        "lgb_model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_test, y_test)],\n",
        "    eval_metric='binary_logloss',\n",
        "    categorical_feature=categorical_columns,\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=50)]\n",
        ")\n",
        "\n",
        "# 4. Predicci贸n y Evaluaci贸n\n",
        "# --------------------------------------------------\n",
        "# Realizar predicciones (ya est谩n en las categor铆as originales)\n",
        "y_pred = lgb_model.predict(X_test)\n",
        "\n",
        "# Calcular m茅tricas directamente con las categor铆as\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, pos_label='Yes')\n",
        "recall = recall_score(y_test, y_pred, pos_label='Yes')\n",
        "f1 = f1_score(y_test, y_pred, pos_label='Yes')\n",
        "\n",
        "# Imprimir m茅tricas\n",
        "print(\"\\nM茅tricas de Evaluaci贸n:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "# 5. Visualizaci贸n\n",
        "# --------------------------------------------------\n",
        "# Matriz de confusi贸n\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=['No Heart Attack', 'Heart Attack'],\n",
        "            yticklabels=['No Heart Attack', 'Heart Attack'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Matriz de Confusi贸n')\n",
        "plt.show()\n",
        "\n",
        "# Curva Precision-Recall (Opcional)\n",
        "\n",
        "y_probs = lgb_model.predict_proba(X_test)[:, 1]\n",
        "precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_probs, pos_label='Yes')\n",
        "ap_score = average_precision_score(y_test, y_probs, pos_label='Yes')\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(recall_curve, precision_curve, label=f'AP Score: {ap_score:.2f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Curva Precision-Recall')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bBrL7HPiTtQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Mal Rendimiento**\n",
        "Como el rendimiento no es tan bueno, ajustaremos el dataset conservando solo las columnas relevantes, ya que las dem谩s est谩n afectando negativamente el rendimiento"
      ],
      "metadata": {
        "id": "XFZ11SntR1Qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columnas_a_eliminar = [\n",
        "    # variable objectivo. as铆 que no debe estar en x para predecirla as铆 misma\n",
        "    'HadHeartAttack',\n",
        "    'State',  # Ubicaci贸n geogr谩fica no es un factor m茅dico directo\n",
        "    'DeafOrHardOfHearing',  # Problemas auditivos no relacionados\n",
        "    'BlindOrVisionDifficulty',  # Problemas visuales no relacionados\n",
        "    'HIVTesting',  # No directamente relacionado con salud card铆aca\n",
        "    'RemovedTeeth',  # Salud dental no es predictor card铆aco\n",
        "    'ChestScan',  # Es un examen, no un factor de riesgo\n",
        "    'FluVaxLast12',  # Vacuna de gripe no es relevante\n",
        "    'PneumoVaxEver',  # Vacuna neumococo no es relevante\n",
        "    'CovidPos',  # Muy reciente para tener datos concluyentes\n",
        "    'HeightInMeters',  # Mejor usar BMI que combina altura/peso\n",
        "    'WeightInKilograms',  # Mejor usar BMI\n",
        "    'DifficultyConcentrating',  # S铆ntoma muy gen茅rico\n",
        "    'DifficultyDressingBathing',  # Movilidad no espec铆fica card铆aca\n",
        "    'DifficultyErrands'  # Movilidad no espec铆fica card铆aca\n",
        "]\n",
        "\n",
        "\n",
        "variables_clave = [\n",
        "\n",
        "\n",
        "    # Factores demogr谩ficos b谩sicos\n",
        "    'Sex',\n",
        "    'AgeCategory',\n",
        "\n",
        "    # Salud general\n",
        "    'GeneralHealth',\n",
        "    'PhysicalHealthDays',\n",
        "    'MentalHealthDays',\n",
        "    'BMI',\n",
        "\n",
        "    # Factores de riesgo cardiovascular\n",
        "    'HadAngina',\n",
        "    'HadStroke',\n",
        "    'HadAsthma',\n",
        "    'HadCOPD',\n",
        "    'HadDiabetes',\n",
        "    'HadKidneyDisease',\n",
        "    'HadArthritis',\n",
        "\n",
        "    # H谩bitos de vida\n",
        "    'SmokerStatus',\n",
        "    'ECigaretteUsage',\n",
        "    'AlcoholDrinkers',\n",
        "    'PhysicalActivities',\n",
        "    'SleepHours',\n",
        "\n",
        "    # Comorbilidades relevantes\n",
        "    'HadDepressiveDisorder',\n",
        "    'HadSkinCancer',  # Algunos estudios muestran correlaci贸n\n",
        "\n",
        "    # Ex谩menes m茅dicos\n",
        "    'LastCheckupTime',\n",
        "    'HighRiskLastYear',\n",
        "\n",
        "    # Dificultades f铆sicas relacionadas\n",
        "    'DifficultyWalking'  # Puede indicar problemas circulatorios\n",
        "]"
      ],
      "metadata": {
        "id": "MRR0pBB-R0vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.unique()"
      ],
      "metadata": {
        "id": "OD0TlV04vxm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.unique()"
      ],
      "metadata": {
        "id": "FY3345u5vx4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "mS4ql40O1R00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.info()"
      ],
      "metadata": {
        "id": "AbaVn2-F10mV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 1. Preparaci贸n de Datos\n",
        "# --------------------------------------------------\n",
        "target_column = \"HadHeartAttack\"\n",
        "\n",
        "# Convertir la variable objetivo a categ贸rica directamente\n",
        "df_imputado[target_column] = df_imputado[target_column].astype('category')\n",
        "\n",
        "# Definir variables predictoras (X) y la variable objetivo (y)\n",
        "X = df_imputado.drop(columns=columnas_a_eliminar)  # Elimina todo de una vez\n",
        "y = df_imputado[target_column]  # Mantenemos como categor铆a\n",
        "\n",
        "# Identificar columnas categ贸ricas en X\n",
        "categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
        "X[categorical_columns] = X[categorical_columns].astype('category')\n",
        "\n",
        "# 2. Divisi贸n de Datos\n",
        "# --------------------------------------------------\n",
        "# Dividir manteniendo las categor铆as originales\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=87, stratify=y\n",
        ")\n",
        "\n",
        "# 3. Configuraci贸n y Entrenamiento del Modelo\n",
        "# --------------------------------------------------\n",
        "lgb_model = lgb.LGBMClassifier(\n",
        "    boosting_type='goss',  # Gradient-based One-Side Sampling\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=15,\n",
        "    num_leaves=31,\n",
        "    min_child_samples=50,\n",
        "    reg_alpha=0.5,\n",
        "    reg_lambda=0.5,\n",
        "    class_weight={'No': 1, 'Yes': 5},\n",
        "    random_state=42,\n",
        "    objective='binary',\n",
        "    metric='aucpr',\n",
        "    n_jobs=-1,\n",
        "    importance_type='gain',\n",
        "    min_data_in_leaf=100,\n",
        "    cat_smooth=20,\n",
        "    extra_trees=True\n",
        ")\n",
        "\n",
        "# Entrenamiento con early stopping (una sola vez)\n",
        "lgb_model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_test, y_test)],\n",
        "    eval_metric='binary_logloss',\n",
        "    categorical_feature=categorical_columns,\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=10)]\n",
        ")\n",
        "\n",
        "# 4. Predicci贸n y Evaluaci贸n\n",
        "# --------------------------------------------------\n",
        "# Realizar predicciones (ya est谩n en las categor铆as originales)\n",
        "y_pred = lgb_model.predict(X_test)\n",
        "\n",
        "# Calcular m茅tricas directamente con las categor铆as\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, pos_label='Yes')\n",
        "recall = recall_score(y_test, y_pred, pos_label='Yes')\n",
        "f1 = f1_score(y_test, y_pred, pos_label='Yes')\n",
        "\n",
        "# Imprimir m茅tricas\n",
        "print(\"\\nM茅tricas de Evaluaci贸n:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "# 5. Visualizaci贸n\n",
        "# --------------------------------------------------\n",
        "# Matriz de confusi贸n\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=['No Heart Attack', 'Heart Attack'],\n",
        "            yticklabels=['No Heart Attack', 'Heart Attack'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Matriz de Confusi贸n')\n",
        "plt.show()\n",
        "\n",
        "# Curva Precision-Recall (Opcional)\n",
        "\n",
        "y_probs = lgb_model.predict_proba(X_test)[:, 1]\n",
        "precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_probs, pos_label='Yes')\n",
        "ap_score = average_precision_score(y_test, y_probs, pos_label='Yes')\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(recall_curve, precision_curve, label=f'AP Score: {ap_score:.2f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Curva Precision-Recall')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Izrl70HJ6kTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.info()"
      ],
      "metadata": {
        "id": "r2sMs7ijRcMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[[\"AgeCategory\"]]"
      ],
      "metadata": {
        "id": "RvpihKcDSYTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Randont Forest Pero con las Filas nulas eliminadas \"HadHearAttack = No\" e imputados en HadHearAttack =SI\"**"
      ],
      "metadata": {
        "id": "--Dq3DiByL_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 1. Crear copia del dataframe original\n",
        "df= df.copy()\n",
        "\n",
        "# 2. Separar los casos positivos (Yes) y negativos (No)\n",
        "positivos = df[df['HadHeartAttack'] == 'Yes']\n",
        "negativos = df[df['HadHeartAttack'] == 'No']\n",
        "\n",
        "# 3. Eliminar filas con valores nulos SOLO en los negativos (No)\n",
        "negativos_limpios = negativos.dropna()\n",
        "\n",
        "# 4. Combinar los positivos completos con los negativos limpios\n",
        "df_Prueba = pd.concat([positivos, negativos_limpios], axis=0)\n",
        "\n",
        "# 5. Imputaci贸n simple para las variables num茅ricas y categ贸ricas restantes\n",
        "# Seleccionar columnas num茅ricas y categ贸ricas\n",
        "numeric_cols = df_Prueba.select_dtypes(include=['float64', 'int64']).columns\n",
        "categorical_cols = df_Prueba.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# Imputar num茅ricos con la mediana (menos sensible a outliers)\n",
        "imputer_num = SimpleImputer(strategy='median')\n",
        "df_Prueba[numeric_cols] = imputer_num.fit_transform(df_Prueba[numeric_cols])\n",
        "\n",
        "# Imputar categ贸ricas con la moda (valor m谩s frecuente)\n",
        "imputer_cat = SimpleImputer(strategy='most_frequent')\n",
        "df_Prueba[categorical_cols] = imputer_cat.fit_transform(df_Prueba[categorical_cols])\n",
        "\n",
        "# Verificaci贸n final\n",
        "print(f\"Registros originales: {len(df_Prueba)}\")\n",
        "print(f\"Registros despu茅s del procesamiento: {len(df_Prueba)}\")\n",
        "print(f\"Distribuci贸n de clases:\\n{df_Prueba['HadHeartAttack'].value_counts(normalize=True)}\")"
      ],
      "metadata": {
        "id": "uzeueeOr0YVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 1. Conteo de clases\n",
        "class_counts = df_Prueba['HadHeartAttack'].value_counts()\n",
        "\n",
        "# 2. Gr谩fico de barras\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=class_counts.index, y=class_counts.values, palette=\"viridis\")\n",
        "plt.title(\"Distribuci贸n de Ataques Card铆acos (Conteo)\")\n",
        "plt.xlabel(\"HadHeartAttack\")\n",
        "plt.ylabel(\"N煤mero de casos\")\n",
        "plt.show()\n",
        "\n",
        "# 3. Gr谩fico de pastel (porcentajes)\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.pie(class_counts, labels=class_counts.index, autopct='%1.1f%%', colors=['#ff9999','#66b3ff'])\n",
        "plt.title(\"Proporci贸n de Ataques Card铆acos\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XHRrH8XmyIr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 1. Preparaci贸n de Datos\n",
        "# --------------------------------------------------\n",
        "target_column = \"HadHeartAttack\"\n",
        "\n",
        "# Convertir la variable objetivo a categ贸rica directamente\n",
        "df_Prueba[target_column] = df_Prueba[target_column].astype('category')\n",
        "\n",
        "# Definir variables predictoras (X) y la variable objetivo (y)\n",
        "X = df_Prueba.drop(columns=columnas_a_eliminar)  # Elimina todo de una vez\n",
        "y = df_Prueba[target_column]  # Mantenemos como categor铆a\n",
        "\n",
        "# Identificar columnas categ贸ricas en X\n",
        "categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
        "X[categorical_columns] = X[categorical_columns].astype('category')\n",
        "\n",
        "# 2. Divisi贸n de Datos\n",
        "# --------------------------------------------------\n",
        "# Dividir manteniendo las categor铆as originales\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=87, stratify=y\n",
        ")\n",
        "\n",
        "# 3. Configuraci贸n y Entrenamiento del Modelo\n",
        "# --------------------------------------------------\n",
        "lgb_model = lgb.LGBMClassifier(\n",
        "    boosting_type='goss',  # Gradient-based One-Side Sampling\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=15,\n",
        "    num_leaves=31,\n",
        "    min_child_samples=50,\n",
        "    reg_alpha=0.5,\n",
        "    reg_lambda=0.5,\n",
        "    class_weight={'No': 1, 'Yes': 5},\n",
        "    random_state=42,\n",
        "    objective='binary',\n",
        "    metric='aucpr',\n",
        "    n_jobs=-1,\n",
        "    importance_type='gain',\n",
        "    min_data_in_leaf=100,\n",
        "    cat_smooth=20,\n",
        "    extra_trees=True\n",
        ")\n",
        "\n",
        "# Entrenamiento con early stopping (una sola vez)\n",
        "lgb_model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_test, y_test)],\n",
        "    eval_metric='binary_logloss',\n",
        "    categorical_feature=categorical_columns,\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=10)]\n",
        ")\n",
        "\n",
        "# 4. Predicci贸n y Evaluaci贸n\n",
        "# --------------------------------------------------\n",
        "# Realizar predicciones (ya est谩n en las categor铆as originales)\n",
        "y_pred = lgb_model.predict(X_test)\n",
        "\n",
        "# Calcular m茅tricas directamente con las categor铆as\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, pos_label='Yes')\n",
        "recall = recall_score(y_test, y_pred, pos_label='Yes')\n",
        "f1 = f1_score(y_test, y_pred, pos_label='Yes')\n",
        "\n",
        "# Imprimir m茅tricas\n",
        "print(\"\\nM茅tricas de Evaluaci贸n:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "# 5. Visualizaci贸n\n",
        "# --------------------------------------------------\n",
        "# Matriz de confusi贸n\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=['No Heart Attack', 'Heart Attack'],\n",
        "            yticklabels=['No Heart Attack', 'Heart Attack'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Matriz de Confusi贸n')\n",
        "plt.show()\n",
        "\n",
        "# Curva Precision-Recall (Opcional)\n",
        "\n",
        "y_probs = lgb_model.predict_proba(X_test)[:, 1]\n",
        "precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_probs, pos_label='Yes')\n",
        "ap_score = average_precision_score(y_test, y_probs, pos_label='Yes')\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(recall_curve, precision_curve, label=f'AP Score: {ap_score:.2f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Curva Precision-Recall')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NprVgey318UL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFj0zbvELlmBGXLf52sJxP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}